// Module included in the following assemblies:
//
// storage/choosing-a-graph-driver.adoc

[id='configuring-graph-drivers-with-the-docker-storage-setup-utility-{context}']
= Configuring graph drivers with the docker storage setup utility

To ease storage configuration, use the `docker-storage-setup` utility, which
automates much of the graph driver configuration details.

.Procedure

. Edit the the *_/etc/sysconfig/docker-storage-setup_* file to specify the device driver:
+
----
STORAGE_DRIVER=devicemapper
----
+
Or
+
----
STORAGE_DRIVER=overlay2
----
+
[NOTE]
====
If using CRI-O specify `STORAGE_DRIVER=overlay`.

With CRI-O, the default `overlay` storage driver uses the `overlay2` optimizations.
====
+
. If you had a separate disk drive dedicated to Docker storage (for example,
*_/dev/xvdb_*), add the following to the *_/etc/sysconfig/docker-storage-setup_*
file:
+
----
DEVS=/dev/xvdb
VG=docker_vg
----

. Restart the `docker-storage-setup` service:
+
----
# systemctl restart docker-storage-setup
----
+
After the restart, `docker-storage-setup` sets up a volume group named
`docker_vg` and creates a thin-pool logical volume. Documentation for thin
provisioning on RHEL is available in the
link:https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html-single/Logical_Volume_Manager_Administration/index.html[LVM
Administrator Guide]. View the newly created volumes with the `lsblk` command:
+
----
# lsblk /dev/xvdb
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvdb 202:16 0 20G 0 disk
└─xvdb1 202:17 0 10G 0 part
  ├─docker_vg-docker--pool_tmeta 253:0 0 12M 0 lvm
  │ └─docker_vg-docker--pool 253:2 0 6.9G 0 lvm
  └─docker_vg-docker--pool_tdata 253:1 0 6.9G 0 lvm
  └─docker_vg-docker--pool 253:2 0 6.9G 0 lvm
----
+
[NOTE]
====
Thin-provisioned volumes are not mounted and have no file system (individual
containers do have an XFS file system), thus they do not show up in `df` output.
====

. To verify that Docker is using an LVM thin pool, and to monitor disk space
use, run the `docker info` command:
+
----
# docker info | egrep -i 'storage|pool|space|filesystem'
Storage Driver: overlay2 <1>
 Backing Filesystem: extfs
----
<1> The `docker info` output when using `overlay2`.
+
----
# docker info | egrep -i 'storage|pool|space|filesystem'
Storage Driver: devicemapper <1>
 Pool Name: docker_vg-docker--pool <2>
 Pool Blocksize: 524.3 kB
 Backing Filesystem: xfs
 Data Space Used: 62.39 MB
 Data Space Total: 6.434 GB
 Data Space Available: 6.372 GB
 Metadata Space Used: 40.96 kB
 Metadata Space Total: 16.78 MB
 Metadata Space Available: 16.74 MB
----
<1> The `docker info` output when using `devicemapper`.
<2> Corresponds to the `VG` you specified in *_/etc/sysconfig/docker-storage-setup_*.

By default, a thin pool is configured to use 40% of the underlying block device.
As you use the storage, LVM automatically extends the thin pool up to 100%. This
is why the `Data Space Total` value does not match the full size of the
underlying LVM device. This auto-extend technique was used to unify the storage
approach taken in both Red Hat Enterprise Linux and Red Hat Atomic Host, which
only uses a single partition.

In development, Docker in Red Hat distributions defaults to a
loopback mounted sparse file. To see if your system is using the loopback mode:

----
# docker info|grep loop0
 Data file: /dev/loop0
----

[IMPORTANT]
====
Red Hat strongly recommends using the overlay2 storage driver in thin-pool mode for production workloads.
====

OverlayFS is also supported for container runtimes use cases as of Red Hat Enterprise Linux
7.2, and provides faster start up time and page cache sharing, which can
potentially improve density by reducing overall memory utilization.
