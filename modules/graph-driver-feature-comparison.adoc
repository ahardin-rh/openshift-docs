// Module included in the following assemblies:
//
// storage/choosing-a-graph-driver.adoc

[id='graph-driver-feature-comparison-{context}']
= Graph driver feature comparisons

Compare the benefits and limitations of available graph drivers.

.Graph driver comparison
|===
|Name |Description |Benefits |Limitations

a|OverlayFS

* overlay
* overlay2
|Combines a lower (parent) and upper (child) filesystem and a working directory
(on the same filesystem as the child). The lower filesystem is the base image,
and when you create new containers, a new upper filesystem is created
containing the deltas.
a|* Faster than Device Mapper at starting and stopping containers. The startup time
difference between Device Mapper and Overlay is generally less than one second.
* Allows for page cache sharing.
|Not POSIX compliant.

|Device Mapper Thin Provisioning
|Uses LVM, Device Mapper, and the dm-thinp kernel module. It differs by removing
the loopback device, talking straight to a raw partition (no filesystem).
a|* There are measurable performance advantages at moderate load and high density.
* It gives you per-container limits for capacity (10G by default).
a|* You have to have a dedicated partition for it.
* It is not set up by default in Red Hat Enterprise Linux (RHEL).
* All containers and images share the same pool of capacity. It cannot be resized
without destroying and re-creating the pool.

|Device Mapper loop-lvm
|Uses the Device Mapper thin provisioning module (dm-thin-pool) to implement
copy-on-write (CoW) snapshots. For each device mapper graph location, thin pool
is created based on two block devices, one for data and one for metadata. By
default, these block devices are created automatically by using loopback mounts
of automatically created sparse files.
|It works out of the box, so it is useful for prototyping and development purposes.
a|* Not all Portable Operating System Interface for Unix (POSIX) features work (for
example, `O_DIRECT`). Most importantly, this mode is unsupported for production
workloads.
* All containers and images share the same pool of capacity. It cannot be resized
without destroying and re-creating the pool.

|===

For better performance, Red Hat strongly recommends using the [overlayFS storage
driver over Device Mapper. However, if you are already using Device Mapper in a
production environment, Red Hat strongly recommends using thin provisioning for
container images and container root file systems. Otherwise, always use
overlayfs2 for Docker engine or overlayFS for CRI-O.

Using a loop device can affect performance issues. While you can still continue
to use it, the following warning message is logged:

----
devmapper: Usage of loopback devices is strongly discouraged for production use.
Please use `--storage-opt dm.thinpooldev` or use `man docker` to refer to
dm.thinpooldev section.
----

== Benefits of using overlayFS or DeviceMapper with SELinux

The main advantage of the OverlayFS graph is Linux page cache sharing among
containers that share an image on the same node. This attribute of OverlayFS
leads to reduced input/output (I/O) during container startup (and, thus, faster
container startup time by several hundred milliseconds), as well as reduced
memory usage when similar images are running on a node. Both of these results
are beneficial in many environments, especially those with the goal of
optimizing for density and have high container churn rate (such as a build
farm), or those that have significant overlap in image content.

Page cache sharing is not possible with DeviceMapper because thin-provisioned
devices are allocated on a per-container basis.

[NOTE]
====
DeviceMapper is the default Docker storage configuration on Red Hat Enterprise Linux.
The use of OverlayFS as the container storage
technology is under evaluation and moving Red Hat Enterprise Linux to OverlayFS as
the default in future releases is under consideration.
====

== Comparing the overlay and overlay2 graph drivers

OverlayFS is a type of union file system. It allows you to overlay one file
system on top of another. Changes are recorded in the upper file system, while
the lower file system remains unmodified. This allows multiple users to share a
file-system image, such as a container or a DVD-ROM, where the base image is on
read-only media.

OverlayFS layers two directories on a single Linux host and presents them as a
single directory. These directories are called layers, and the unification
process is referred to as a union mount.

OverlayFS uses one of two graph drivers, *overlay* or *overlay2*. As of Red Hat
Enterprise Linux 7.2, *overlay*
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/7.2_release_notes/technology-preview-file_systems[became
a supported graph driver]. As of Red Hat Enterprise Linux 7.4, *overlay2*
link:https://access.redhat.com/solutions/2908851[became supported]. SELinux on
the docker daemon became supported in Red Hat Enterprise Linux 7.4. See the
link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/?version=7[Red
Hat Enterprise Linux release notes] for information on using OverlayFS with your
version of RHEL, including supportability and usage caveats.

The *overlay2* driver natively supports up to 128 lower OverlayFS layers but,
the *overlay* driver works only with a single lower OverlayFS layer. Because of this capability, the *overlay2* driver provides better performance
for layer-related Docker commands, such as `docker build`, and consumes fewer inodes on the backing filesystem.

Because the *overlay* driver works with a single lower OverlayFS layer, you cannot implement multi-layered images as multiple OverlayFS layers.
Instead, each image layer is implemented as its own directory under *_/var/lib/docker/overlay_*.
Hard links are then used as a space-efficient way to reference data shared with lower layers.
